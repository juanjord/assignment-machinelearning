---
title: "Practice Machine Learning - Assignment"
author: "JJ"
date: "23/8/2020"
output: 
  html_document:
    keep_md: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Reading the data and packages
```{r}
training <- read.csv("pml-training.csv", na.strings = c("NA",""))
testing <- read.csv("pml-testing.csv", na.strings = c("NA",""))

library(caret)
library(randomForest)
library(plotly)
library(dplyr)
```


# Choosing variables
```{r}
str(training)
```

The structure of the data shows that there are several variables with a lot of missing values, therefore these will be taken out of the analysis in order to build a model.

```{r}
# Training dataset
training <- training %>% select(num_window:total_accel_belt, gyros_belt_x:total_accel_arm, gyros_arm_x:magnet_arm_z, roll_dumbbell:yaw_dumbbell, total_accel_dumbbell, gyros_dumbbell_x:yaw_forearm, total_accel_forearm, gyros_forearm_x:classe)
```


In this case, the training and testing dataset should have the same variables, so I select these variables for the testing data as well. 

```{r}
# Testing dataset
testing <- testing %>% select(num_window:total_accel_belt, gyros_belt_x:total_accel_arm, gyros_arm_x:magnet_arm_z, roll_dumbbell:yaw_dumbbell, total_accel_dumbbell, gyros_dumbbell_x:yaw_forearm, total_accel_forearm, gyros_forearm_x:magnet_forearm_z)
```


The "classe" variable needs to be read as a factor variable, since there are five total classes (A,B,C,D,E,F). On the other hand,there is a total of 53 predictors. 
```{r}
str(training)
training$classe <- factor(training$classe)
```


# Choosing a method


I will be using Random Forests since generally this is a good method for predictions. Also, cross validation is necessary in order to evaluate how well this method works. In this case, I use the k-fold method, and since the dataset has almost 20000 observations, a value for k=10 is acceptable.

```{r}
set.seed(1240)
folds <- createFolds(1:nrow(training), k = 10)
```


# Cross validation

A for loop can be made to store the level of accuracy that each of the folds has, in order to average these 10 different values to see how good the model is (and the variables) to determine the class.

```{r}
# Cross Validation

ACC<-c()
# Loop to calculate the accuracy of the model
for (i in 1:10) {
    test.fold <- training[folds[[i]],]
    train.fold <- training[-folds[[i]],]
    mod <- randomForest(classe ~ ., method="class", data = train.fold)
    pred <- predict(mod, test.fold)
    t <- confusionMatrix(pred, test.fold$classe)
    ACC[i] <- t$overall[1] # Storing the accuracy for each fold
}
plot(ACC, type = "b", col = "blue", pch = 18, xlab = "fold", ylab = "accuracy")
```


```{r}
# Mean accuracy of the 10 folds
mean(ACC)
```


The plot shows that all of these values are close to 0.99 for each iteration, which means that the model has a pretty good accuracy. Also, the mean is 0.998, very close to 1.



# Predicting the new data (20 observations)
```{r}
mod.rf <- randomForest(classe ~ ., method="class", data = training)
pred2 <- predict(mod.rf, testing)
```

Finally, the prediction for the 20 observations can be made through the model, although in this case, the model has to use the whole training dataset instead of just a fold.


```{r}
# classification for the 20 observations
pred2
```


